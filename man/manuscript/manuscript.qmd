---
title: "quicR: An R Library for Streamlined Data Handling of Real-Time Quaking Induced Conversion Assays"
authors:
    - id: GR
      name: Gage Rowden
      orcid: 0000-0002-7517-0480
      email: rowde002@umn.edu
      affil-id: 1,2,3
      roles:
          Conceptualization
          Methodology
          Software
          Validation
          Formal analysis
          Investigation
          Resources
          Data curation
          Writing--original draft preparation
          Writing--review and editing
          Visualization
          Supervision
          Project administration
      corresponding: true
    - id: PL
      name: Peter Larsen
      orcid: 0000-0002-3634-3625
      email: plarsen@umn.edu
      affil-id: 1,2,3
      roles:
          Supervision
          Project administration
          Funding
affiliations:
  - id: 1
    name: Department of Veterinary and Biomedical Sciences, University of Minnesota, St. Paul, MN 55108, USA.
  - id: 2
    name: Minnesota Center for Prion Research and Outreach, University of Minnesota, St. Paul, MN 55108, USA.
  - id: 3
    name: Priogen Corp., St. Paul, MN 55114, USA.
      
abstract: Real-time quaking induced conversion (RT-QuIC) has quickly become a valuable diagnostic tool for protein misfolding disorders such as Creutzfeldt-Jakob disease and Parkinson's disease. Given that the technology is relatively new, academic and industry standards for quality filtering data and high throughput analysis of results have yet to be fully established. The open source R library, quicR, was developed to provide a standradized approach to RT-QuIC data analysis. quicR provides functions, which can be easily integrated into existing R workflows, for data curation, analysis, and vizualization.
keywords: quicR, R, library, RT-QuIC, prion, diagnostics, CJD, Parkinson's
format: 
  pdf:
    papersize: letter
    documentclass: article
    fontsize: 12pt
    geometry:
      - top=20mm
      - right=20mm
      - bottom=20mm
      - left=20mm
    fig-pos: "H"
    # keep-tex: true
    template-partials:
      - title.tex
    include-in-header:
      - text: \usepackage{tabularray}
      - text: \usepackage{float}
      - text: \floatplacement{table}{H}
      - text: \usepackage[noblocks]{authblk}
      - text: \renewcommand*{\Authsep}{, }
      - text: \renewcommand*{\Authand}{, }
      - text: \renewcommand*{\Authands}{, }
      - text: \renewcommand\Affilfont{\small}
  html: default
crossref: 
  fig-title: "**Figure**"
  tbl-title: "**Table**"
mainfont: Calibri
bibliography: references.bib
bibliographystyle: apa
---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(quicR)
library(dplyr)
library(tidyr)
library(kableExtra)
library(readxl)
library(readr)
library(ggplot2)
library(reshape2)

file <- "../../inst/extdata/input_files/test4.xlsx"
file_384 <- "../../inst/extdata/input_files/test384.xlsx"
sample_file <- "../../inst/extdata/BMG_formatting/plate_layout.csv"
path <- "../../inst/extdata/BMG_formatting"
```


# Introduction
Real-time quaking induced conversion (RT-QuIC) is a cutting-edge diagnostic assay that has garnered significant attention for its ultra-sensitive detection of misfolded protein aggregates [@Wilham2010; @Atarashi2011]. The assay works by converting a recombinant protein substrate into an amyloid aggregate in the presence of a misfolded seed [@Wilham2010; @Orru2012; @Orru2017; @Orru2015; @Bongianni2019; @Dassanayake2016; @Hwang2018; @Groveman2018; @Metrick2020]. The assay's sensitivity and specificity make RT-QuIC a promising tool for diagnosing diseases such as prion disorders and other protein misfolding pathologies [@Fiorini2020; @Franceschini2017; @Picasso-Risso2022; @Holz2021]. However, the relatively recent development and novelty of the assay have left a gap in widely accepted academic and industry standards for data analysis and interpretation [@Rowden2023].

To address this gap, we introduce quicR, an open-source library, developed in R [@R2024], dedicated to the cleaning, analysis, and visualization of RT-QuIC data. By consolidating key metrics and providing robust analytical tools, quicR aims to standardize the analysis pipeline and foster reproducibility within the field of quaking induced assays including related assays such as Nano-QuIC [@Christenson2023] and Micro-QuIC [@Lee2024]. quicR is designed with both researchers and diagnosticians in mind, providing a user-friendly interface that integrates seamlessly with existing R workflows.

While universal diagnostic criteria for RT-QuIC have yet to be established, certain analytical metrics have emerged as valuable tools for interpreting assay results and kinetics. These include:

1.    Time-to-threshold (TtT): The time required for the fluorescence signal to exceed a predefined threshold [@Orru2015].
2.    Rate of amyloid formation (RAF): A measure of the kinetics of aggregate growth, which provides insight into the relative quantity of misfolded seed [@Gallups2022].
3.    Maxpoint ratio (MPR): A ratio-based metric measuring peak normalized fluorescence intensities [@Rowden2023].
4.    Maximum slope (MS): The steepest rate of fluorescence increase, reflecting the most rapid phase of aggregation [@Henderson2015].

Together, these metrics enable researchers to characterize the kinetics of RT-QuIC reactions comprehensively, enhancing the rigor and reliability of diagnostic decisions.

In addition to analytical tools, quicR provides flexible and customizable visualization capabilities. Leveraging the powerful ggplot2 library [@ggplot2016], quicR enables users to generate high-quality, publication-ready figures. These visualizations can be further customized using the intuitive '+' syntax of ggplot2, allowing for tailored presentations of RT-QuIC data.

By combining standardized metrics, advanced visualization tools, and a commitment to open source science, quicR serves as a foundational resource for the growing RT-QuIC community. Its goal is to empower researchers to analyze and present their data with clarity, consistency, and cohesion.

# Methods

## Dependencies
This package requires the following dependencies: dplyr, ggplot2, stringr, tidyr, janitor, openxlsx, readxl, reshape2, and slider. The packages, openxlsx [@openxlsx2024] and readxl [@readxl2023], were fundamental to performing initial data handling of raw data Excel files. The tidyverse packages (dplyr, ggplot2, stringr, and tidyr), were vital for writing easy-to-read code and for data visualization [@tidyverse2019]. The janitor package [@janitor2024] has data cleaning functions which were useful when importing data from Excel. The slider package [@slider2024] provides tools which apply some function to a moving window which was crucial for determining the approximate derivative of raw data.

\newpage

## Input Formatting
The FLUOstar® Omega series microplate readers (BMG Labtech, Ortenberg, Germany) are by far the most common readers used for RT-QuIC. As such, their analysis software, MARS, is integral to data input for this package. MARS exports data into Excel workbooks, and for many of the quicR functions to work together, the workbooks must be formatted correctly. In MARS, select "Excel Report", and a pop-up window ([@fig-mars]) will appear. The following options must be checked for proper output: "Microplate View", "Table view", "Transpose table", and "Add test run information".

![Excel export settings in MARS. Ensure that "Microplate View", "Table view", "Transpose table", and "Add test run information" are selected.](images/MARS_settings.png){#fig-mars height=50%}

After clicking "Export report to Excel", an Excel workbook is created such as in [@fig-marsoutput]. This workbook will contain two sheets, the first with the microplate views and the second with the table view. The first sheet will have all of the relevant metadata while the second sheet will have all of the raw fluorescent data.

:::{#fig-marsoutput layout-nrow=2}
![Microplate view Excel spreadsheet. This is the first spreadsheet in the workbook and contains the plate layouts of any data the user exported from MARS. Typically, there is a header containing metadata followed by a number of named matrices.](images/sheet1.png){#fig-sheet1 height=50%}

![Table view Excel spreadsheet. This is the second spreadsheet in the workboook and contains columns of each well and time points. Each cell is a raw fluorescent value. This spreadsheet will also contain a header, but was excluded from this figure.](images/sheet2.png){#fig-sheet2 height=50%}

Excel workbook exported from MARS.
:::

## Key Metrics and Calculations
quicR has functions for calculating TtT, MPR, and MS (vizualized in [@fig-metrics]). There is no dedicated function for RAF since it can be expressed as the inverse of TtT, and can therefore be calculated separately as in the example in the Calculations section.

TtT is calculated by iterating through each sample until a value is greater than the user-supplied threshold. It then determines the intersection between the previous and current read times and the threshold. If no value was found larger than the threshold, the total reaction run-time is returned.

MPR is defined as the maximum fluorescence divided by the background fluorescence [@Rowden2023]. Thus, in order to calculate, the raw data must first be normalized against the background. This is done by the user choosing a cycle for background determination, and then dividing each read by that value. The MPR is taken as the max value of the normalized data.

MS is determined by approximating the maximum of the derivative of the raw data and is typically reported in units of $\Delta$RFU/h (i.e. the change in relative fluorescent units per hour). Slopes are calculated using differences between two data points within the range of a sliding window. While this slightly reduces the accuracy of the approximation, the improvement in computation time exceeded the loss in resolution.

![Example graph highlighting the calculated metrics described above. The red curve represents a raw data curve that has been normalized against background. The maxpoint ratio is calculated as the maximum fluorescent value achieved in the normalized raw data. Time-to-threshold is determined as the time required to cross a given threshold (in this example, the threshold is set at 0.2). The blue curve represents the approximate derivative of the raw data, and max slope is determined as the maximum of the derivative.](images/metric_example.png){#fig-metrics}

# Development
![Workflow hierarchy of the quicR package. Blue nodes indicate steps where BMG software is needed. Purple nodes indicate functions dedicated to handling metadata. Red nodes are functions that acquire and manipulate raw data. Orange nodes are functions which calculate some metric. Finally, yellow nodes represent data analysis endpoints.](images/workflow2.png){#fig-workflow}

<!-- quicR was developed to address the growing need for efficient data conversion, analysis, and visualization of RT-QuIC data. With a focus on usability and reproducibility, the package is designed to standardize workflows and ensure compatibility across multiple laboratories. Its primary input format is data exported as Excel workbooks from the proprietary MARS software (BMG Labtech, Ortenberg, Germany), providing seamless integration with existing experimental workflows. -->

<!-- ## Design Philosophy -->

<!-- The development of quicR was guided by three key principles: -->

<!-- 1.    Usability: The package features intuitive functions and a user-friendly interface that cater to both novice and experienced R users. -->
<!-- 2.    Reproducibility: quicR incorporates rigorous testing and version control to ensure consistent performance across operating systems and environments. -->
<!-- 3.    Flexibility: Recognizing the diverse needs of the RT-QuIC community, the package supports customizable workflows and outputs, allowing users to tailor analyses to their specific research goals. -->

## Version Control and Collaboration
The project was managed using Git for version control, enabling efficient tracking of changes and fostering collaborative development. The source code is hosted on GitHub (https://github.com/gage1145/quicR), ensuring transparency and encouraging contributions from the research community. GitHub Actions were employed to automate testing and deployment workflows, verifying that the package remains compatible with current versions of R and its dependencies.

## Testing and Validation
Robust testing protocols were implemented to guarantee the reliability of quicR’s functionality. Each function was subjected to comprehensive unit testing using the testthat package [@testthat2011]. These tests ensure that key metrics, data manipulation routines, and visualization tools perform as expected, even as the package evolves. Each new build of quicR must pass these tests in order to be released.

## Core Functionalities
The functionality of quicR is centered around three primary objectives:

1.    Data Curation: Functions for importing, cleaning, and normalizing raw RT-QuIC data to streamline pre-processing.
2.    Metric Calculations: Tools for deriving critical metrics, such as time-to-threshold (TtT), maxpoint ratio (MPR), and maximum slope (MS), to enable standardized data interpretation.
3.    Visualization: High-quality plotting capabilities powered by the ggplot2 library, enabling users to create publication-ready figures with minimal effort.

<!-- ## Continuous Development -->
<!-- quicR is an ongoing project, with future updates planned to expand its functionality and improve its user experience. Planned enhancements include: -->

<!-- -   Support for alternative data formats to broaden compatibility. -->
<!-- -   Inclusion of analysis pipeline given popular research community methods. -->
<!-- -   Additional metrics and visualization options to meet the evolving needs of the RT-QuIC community. -->
<!-- -   Integration with R Shiny for interactive data exploration. -->

<!-- By combining rigorous development practices, a commitment to open science, and a focus on community-driven improvements, quicR serves as a foundational tool for advancing the analysis of RT-QuIC data. -->

\newpage

# Implementation
The implementation of the quicR package encompasses several streamlined processes designed to facilitate data input, cleaning, transformation, and analysis of real-time fluorescence data. This section provides a comprehensive guide to utilizing the package's key functionalities, detailing how to:

1.  Format and input sample data into Omega control software (BMG Labtech, Ortenberg, Germany).
2.  Extract, clean, and organize metadata and raw fluorescence data.
3.  Apply transformations and normalization to raw data for downstream analysis.
4.  Calculate critical analytical metrics, such as time-to-threshold (TtT), rate of amyloid formation (RAF), maxpoint ratio (MPR), and maximum slope (MS).

These steps are designed to enhance reproducibility, minimize manual data handling, and enable seamless integration with the MARS software workflow. Through practical examples, this section illustrates how each function operates, along with expected input and output formats, ensuring clarity and ease of use for researchers.

## Input of Sample IDs into Omega Control Software
The Omega control software allows input of a TXT file containing sample IDs, dilution factors, and their well locations. This file is uniquely formatted, and not easily reproduced manually. The function, "BMG_format", allows for input of a CSV file containing the plate layout (see @tbl-layout for proper formatting), and exports the formatted TXT file. The file can then be imported into the control software before running.

:::{#tbl-layout tbl-pos="ht"}
```{=latex}
\begin{tblr}{
  colspec = {|c|cccccccccccc|}, 
  row{1} = {font=\bfseries}, 
  column{1} = {font=\bfseries}, 
  rowhead = 1
}
  \hline
   & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\ 
  \hline
  A & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  B & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  C & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  D & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  E & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  F & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  G & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  H & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  \hline
\end{tblr}
```
Example CSV file plate layout for input into the "BMG_format" function. The top left corner should be cell "A1" in the CSV file. The top numbered row and the left-most lettered column should never be altered.
:::

\newpage

### Formatted Plate Layout for MARS Input
The function, "BMG_format", includes the logical argument "write_file". If TRUE, it will create a TXT file. The path can be given to the "save_path" argument, and the file name can be supplied to the "save_name" argument. The text file will be formatted as follows, and can be imported into MARS.

```{r, eval=FALSE, message=FALSE}
BMG_format(file, write_file = TRUE, save_path = "", save_name = "formatted.txt")
```
```
A1  P      P
B1  P      P
C1  P      P
D1  P      P
E1  N      N
F1  N      N
G1  N      N
H1  N      N
A2  X1     S01
B2  X1     S01
C2  X1     S01
D2  X1     S01
E2  X1     S01
F2  X1     S01
G2  X1     S01
H2  X1     S01
```

## Data Cleaning and Transformation
The MARS software (BMG Labtech, Ortenberg, Germany) exports real-time data as an Excel workbook. Typically, the first sheet in the workbook will include microplate views of both raw data and metadata; however, the metadata on this page is what is most useful for downstream processes. Those tables include the "Sample IDs" and "Dilutions" tables (if dilutions were included in the MARS export). For much of the downstream analysis, it is crucial that the "Sample IDs" table was exported from MARS. If there is no table, the user can simply add it manually (see [@fig-sheet1] for proper formatting).

### Retrieving Metadata
The metadata is defined as either sample-dependent or -independent. Sample-dependent metadata includes information such as sample IDs and dilution factors, whereas sample-independent metadata includes the date, time, reaction ID, etc.

The dependent metadata can be retrieved using the "organize_tables" and "convert_tables" functions. The former returns a list of tables, and the latter converts each table into a column in a single data frame. See @tbl-tables and @tbl-converted for example outputs of these functions.
\newpage
```{r, echo=FALSE, message=FALSE}
tabs <- organize_tables(file)
```
```{r, eval=FALSE}
organize_tables(file)
```

```{r, echo=FALSE, as.is=TRUE}
#| label: tbl-tables
#| tbl-cap: Sample IDs and Dilution Factors. Note that these are the same tables shown in [@fig-sheet1].
rename <- function(x) {
  names(x) <- 1:12
  rownames(x) <- LETTERS[1:8]
  return(x)
}

tabs$`Sample IDs` |>
  as.data.frame() |>
  rename() |>
  kable(
    format = "latex",
    booktabs = TRUE,
    linesep = "",
    align = rep("c", 12)
  ) |>
  column_spec(1, border_left = TRUE, border_right = TRUE, bold = TRUE) |>
  column_spec(13, border_right = TRUE) |>
  row_spec(0, bold = TRUE) |>
  kable_styling(full_width = TRUE, font_size = 10)

tabs$Dilutions |>
  as.data.frame() |>
  rename() |>
  kable(
    format = "latex",
    booktabs = TRUE,
    linesep = "",
    align = rep("c", 12)
  ) |>
  column_spec(1, border_left = TRUE, border_right = TRUE, bold = TRUE) |>
  column_spec(13, border_right = TRUE) |>
  row_spec(0, bold = TRUE) |>
  kable_styling(full_width = TRUE, font_size = 10)
```

```{r, eval=FALSE}
organize_tables(file) |> convert_tables()
```
```{r, echo=FALSE, message=FALSE}
#| label: tbl-converted
#| tbl-cap: The tables extracted from the "organize_tables" function are converted into columns in a dataframe. This format is much more compatible with downstream analysis.
tabs <- convert_tables(tabs)
tabs |> head(12) |> kable(row.names = FALSE, align = c("c", "c"))
```

\newpage

### Sample Locations
Samples locations can be extracted based on their well ID. The "get_sample_locations" function accepts additional arguments if dilution factors were exported from MARS. The "dilution_bool" argument can be set to TRUE if dilutions are to be included in the output. The "dilution_fun" argument will supply a function for transforming the dilution factors (e.g. if the user would want to perform a log transformation). The user can supply a delimiter using the "sep" function. The output of this function is critical as an argument in the "plate_view" function which is further explained in the visualization section.

```{r, eval=FALSE}
get_sample_locations(
  file, 
  dilution_bool = TRUE, 
  dilution_fun = function(x) -log10(x), 
  sep = "_"
)
```
```{r, echo=FALSE}
#| label: tbl-samp-loc
#| tbl-cap: Well locations of each sample. Included in this is the dilution factor which has been transformed as the negative log of original dilution values. This table is important for input into the "plate_view" function.
get_sample_locations(
  file, dilution_bool = TRUE, dilution_fun = function(x) -log10(x), sep = " "
) |>
  head(12) |>
  kable(row.names = FALSE, align = c("c", "c"))
```

\newpage

### Sample-Independent Metadata
The independent metadata can be retrieved using the "get_meta" function. This data is included in the header of the excel workbook.

```{r, eval=FALSE}
get_meta(file)
```
```{r, echo=FALSE, message=FALSE}
#| label: tbl-meta
#| tbl-cap: Metadata which is included in the header of the Excel file.
get_meta(file) |>
  mutate_at("Meta_info", ~gsub("\\\\", "/", .)) |>
  kable(row.names = FALSE)
```

## Retrieving and Manipulating Raw Data
The raw, real-time data is typically found on the second sheet of the Excel workbook exported from MARS. There are three functions dedicated to the retrieval and cleaning of raw data.

1.  `get_real`: Retrieves the raw data from the Excel file, and outputs it as a dataframe.
2.  `transpose_real`: Swaps the rows and columns which makes some downstream analyses easier.
3.  `normalize_RFU`: normalizes the raw data by dividing each read by background fluorescence at a given cycle.

\newpage

### Retrieve Raw Data
Raw data can be retrieved using the "get_real" function. The logical argument, "ordered", indicates whether the user would prefer the columns to be ordered by well or by sample ID. By default, it is FALSE which will order the data by well. This should almost always be the case for easier integration with other downstream functions. Additionally, since there can be more than one instance of real-time data (depending on if the user added some calculations in MARS), "get_real" returns a list of dataframes. Therefore, the output must be indexed to access the data frame of interest.
```{r, eval=FALSE}
#| label: tbl-raw
#| tbl-cap: Raw data retrieved directly from the MARS output file.
get_real(file)[[1]]
```
```{r, echo=FALSE}
columns <- 8
df_ <- get_real(file, ordered = FALSE)[[1]] |> as.data.frame()
df_[1:11, 1:columns] |> kable(row.names = FALSE, align = rep("c", columns))
```

\newpage

### Transpose Raw Data
This data is structured such that each sample is its own column (variable) and each row (observation) is a time point. While this format is technically correct, a transposed format is more ideal for some downstream manipulation. This operation is performed using the function, "transpose_real". After transposition, each time point is an individual column (variable), and each sample is an individual row (observation).

```{r, eval=FALSE}
get_real(file)[[1]] |> transpose_real()
```
```{r, echo=FALSE}
#| label: tbl-transposed
#| tbl-cap: Transposed raw data. This converted the data columns to rows and the rows to columns.
columns <- 12
transpose_real(df_)[1:7, 1:columns] |> 
  kable(row.names = FALSE, align = rep("c", columns))
```

### Normalize Raw Data
The function "normalize_RFU" will convert the raw data into a background normalized data set. The function includes two additional arguments, "bg_cycle" (the cycle which will be used as the background fluorescence value) and "transposed" (if FALSE, will make a call to the "transpose_real" function). Note that the fourth time point is all "1's" since this was designated the background cycle.

```{r, eval=FALSE}
get_real(file)[[1]] |> normalize_RFU(transposed = FALSE)
```
```{r, echo=FALSE, message=FALSE}
#| label: tbl-norm
#| tbl-cap: Normalized raw data Note that the "transposed" argument was set to false, so a call was made to the "transpose_real" function.
df_norm <- normalize_RFU(df_, bg_cycle = 4, transposed = FALSE)

df_norm[1:7, 1:columns] %>%
  mutate_at(2:ncol(.), ~round(as.numeric(.), 2)) %>%
  kable(row.names = FALSE, align = rep("c", columns))

df_norm <- df_norm |>
  mutate(`Sample IDs` = tabs$`Sample IDs`)
```

\newpage

## Calculations
There are three analytical metrics with dedicated functions: time-to-threshold (TtT), maxpoint ratio (MPR), and maximum slope (MS). The rate of amyloid formation does not have a designated function since it is simply the reciprocal of the time-to-threshold (1/TtT). Each function below accepts input from the "transpose_real" or the "normalize_RFU" functions. See @tbl-metrics for an example of the output of these functions.

### Thresholds
Many publications have different methods of determining thresholds. By convention, the most popular method is to take the average background fluorescence of the every well and add some multiple of standard deviations [@Rowden2023]. The quicR package provides the "calculate_threshold" function for this purpose. A value can be provided as the optional argument, "multiplier", which will be applied to the standard deviation.
```{r, eval=FALSE}
get_real(file)[[1]] |> 
  calculate_threshold(method = "stdev", multiplier = 10)
```
```{r, echo=FALSE}
calculate_threshold(df_, method = "stdev", multiplier = 10)
```

### Time-to-Threshold & Rate of Amyloid Formation
TtT is calculated using the "calculate_TtT" function. The function must be supplied a threshold; default value is 2 (i.e. twice the background fluorescence if the data is normalized). A starting column should also be given as an integer; default value is 3. This is essentially asking how many columns of metadata are included before the fluorescence reads begin. 

TtT is calculated by iterating through each row and checking if a value is greater than the threshold. If the value is greater, the slope of the previous time-point to the current time-point is calculated, and the time intersection of the current read is returned.

### Maxpoint Ratio
MPR is calculated by the "calculate_MPR" function. Data must be normalized in order to derive this metric. In a normalized data set, the MPR is simply the maximum value achieved during the run. Raw data can be passed to this function, but the argument, "data_is_norm", must be set to TRUE. This will pass the raw data to "normalize_RFU" before calculating the MPR values.

### Maximum Slope
MS is calculated by the "calculate_MS" function. The function iterates through each row using a rolling window which can be adjusted (default value is 3). Given the window size, the slope is calculated based on change in fluorescence divided by the range of the window. The MS is simply the largest slope value recorded. The units are typically reported as $\Delta$RFU/h.

\newpage

```{r, eval=FALSE}
df_norm <- get_real(file)[[1]] |> 
  normalize_RFU()

data.frame("Sample IDs" = tabs$`Sample IDs`) |>
  mutate(
    Dilutions = -log10(tabs$dilutions),
    MPR = calculate_MPR(df_norm, start_col = 3, data_is_norm = TRUE),
    MS  = calculate_MS(df_norm, data_is_norm = TRUE),
    TtT = calculate_TtT(df_norm, threshold = 2, start_col = 3),
    RAF = 1 / TtT
  )
```
```{r, echo=FALSE}
#| label: tbl-metrics
#| tbl-cap: Calculated metrics. Each row indicates an individual well. This table shows only the first 12 wells which correspond to the top row of the microplate.
#| tbl-cap-location: top
df_analyzed <- data.frame("Sample IDs" = tabs$`Sample IDs`, check.names = FALSE) |>
  mutate(
    Dilutions = -log10(tabs$Dilutions |> na.omit() |> as.numeric()),
    MPR = calculate_MPR(df_norm, start_col = 3, data_is_norm = TRUE) |> round(2),
    MS = calculate_MS(df_norm, data_is_norm = TRUE) |> round(3),
    TtT = calculate_TtT(df_norm, threshold = 2, start_col = 3) |> round(2),
    RAF = (1 / TtT) |> round(4)
  )
df_analyzed|>
  head(12) |>
  kable(row.names = FALSE, align = rep("c", ncol(df_analyzed))) |>
  row_spec(0, bold = TRUE)
```

\newpage

## Visualization

### Plate View
The "plate_view" function requires *un-transposed* data and sample locations as arguments. It also has an argument for plate type which will either be 96 or 384.
```{r, warning=FALSE}
#| fig-width: 12
#| fig-height: 8
#| fig-cap: Plate-view analog of a 96-well microplate. Each facet in the 8x12 grid shows the real-time curves of an RT-QuIC reaction. The numbers underneath the sample IDs are the dilution factors.
sample_locations <- get_sample_locations(
  file, 
  dilution_bool = TRUE,
  dilution_fun = function(x) -log10(x)
)

plate_view(df_, sample_locations)
```

\newpage

### Summary Plots
```{r, warning=FALSE, message=FALSE}
#| fig-width: 8
#| fig-height: 6.5
df_analyzed |>
  melt(id.vars = c("Sample IDs", "Dilutions")) |>
  mutate_at("Dilutions", as.factor) |>
  
  ggplot(aes(`Sample IDs`, value, fill = Dilutions)) +
    geom_boxplot() +
    facet_wrap(~variable, scales = "free_y") +
    theme(
      legend.position = "bottom",
      strip.text = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
      axis.title = element_blank()
    )
```

# Usage

## Installation
```{r, eval=FALSE}
# Latest CRAN release
install.packages("quicR")

# Development version
devtools::install_github("gage1145/quicR")
```

## Example Workflow
```{r, eval=FALSE}
library(quicR)

# Import and Format Data --------------------------------------------------

file <- "example.xlsx"
raw <- get_real(file)[[1]]
normal <- normalize_RFU(raw, transposed = FALSE)
meta <- file |>
  organize_tables() |>
  convert_tables()

# Calculate Key Metrics ---------------------------------------------------

mpr <- calculate_MPR(normal, data_is_norm = TRUE)
ms  <- calculate_MS(normal, data_is_norm = TRUE)
ttt <- calculate_TtT(normal, threshold = 2)

analyzed <- data.frame(
  `Sample IDs`                = meta$`Sample IDs`,
  `Dilutions`                 = meta$Dilutions,
  `Maxpoint Ratio`            = mpr,
  `Max Slope`                 = ms,
  `Time-to-Threshold`         = ttt,
  `Rate of Amyloid Formation` = 1 / ttt
)
```
\newpage

# Discussion
The quicR package represents a significant advancement in the standardization and reproducibility of RT-QuIC data analysis. By integrating key metrics such as time-to-threshold (TtT), maxpoint ratio (MPR), and maximum slope (MS), quicR addresses critical gaps in the field, providing researchers and diagnosticians with a robust toolkit for interpreting complex fluorescence data.

One of the primary strengths of quicR lies in its flexibility and user-centric design. The package leverages R’s powerful ecosystem, including the tidyverse and ggplot2, to streamline workflows and create high-quality, customizable visualizations. This ensures accessibility for a wide range of users, from experienced data scientists to wet-lab researchers unfamiliar with programming. Additionally, the incorporation of open-source principles allows the broader scientific community to contribute to its development, fostering innovation and adaptability.

Despite these strengths, there are limitations to consider. Currently, quicR is tailored to data exported from the MARS software, which may limit its applicability to researchers using alternative fluorescence readers. Future iterations of the package could expand compatibility by incorporating functions to handle diverse data formats. Furthermore, while quicR includes robust visualization tools, users seeking highly specialized plots may require additional customization beyond the package’s default capabilities.

Another avenue for improvement lies in the standardization of RT-QuIC diagnostic criteria. quicR provides tools to calculate key metrics, but consensus on thresholds and interpretations remains a challenge for the field. Collaborative efforts among researchers and clinicians are necessary to define universal criteria, enabling quicR to fully realize its potential as a diagnostic aid. Diagnostic determinations could easily be built into the library, but a larger consensus within the research community will need to be reached to warrant inclusion.

# Conclusion
quicR offers a powerful solution for the cleaning, analysis, and visualization of RT-QuIC data, addressing critical needs in a rapidly evolving field. By enabling consistent data handling and interpretation, quicR lays the groundwork for improved diagnostic consistency and reproducibility. The package's open-source nature ensures that it will continue to evolve, integrating new insights and technologies as they emerge.

As RT-QuIC technology advances, tools like quicR will play a pivotal role in bridging the gap between assay development and practical application. By equipping researchers with reliable, standardized tools, quicR not only supports the study of prion and protein misfolding disorders but also serves as a model for the development of software solutions in other diagnostic fields.

# Acknowledgments
Special thanks to Beni Altmann at The Comprehensive R Archive Network (CRAN) for help during the submission process to CRAN. We thank Tiffany Wolf and Marc Schwabenlander for their support through the Minnesota Center for Prion Research and Outreach. We would like to acknowledge Suzanne Stone and Sarah Gresch for maintaining lab operations.

# Funding
This work was funded through the Legislative-Citizen Commission on Minnesota Resources (LCCMR).

# References
::: {#refs}
:::

# Appendix

