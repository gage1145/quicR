---
title: "quicR: An R Library for Streamlined Data Handling of Real-Time Quaking Induced Conversion Assays"
author:
    - name: 
        given: Gage
        family: Rowden
        id: GR
        orcid: 0000-0002-7517-0480
        email: rowde002@umn.edu
        affiliation:
          name: University of Minnesota
          city: Saint Paul
          state: MN
        roles:
          Conceptualization
          Methodology
          Software
          Validation
          Formal analysis
          Investigation
          Resources
          Data curation
          Writing--original draft preparation
          Writing--review and editing
          Visualization
          Supervision
          Prject administration
        corresponding: true
    - name:
        given: Peter
        family: Larsen
        id: PL
        orcid: 0000-0000-0000-0000
        email: plarsen@umn.edu
        affiliation:
          name: University of Minnesota
          city: Saint Paul
          state: MN
        roles:
          Supervision
          Project administration
          Funding
      
abstract: Real-time quaking induced conversion (RT-QuIC) has quickly become an emerging diagnostic tool for protein misfolding disorders such as Creutzfeldt-Jakob disease and Parkinson's disease. Given that the technology is still relatively new, academic and industry standards for cleaning data and analyzing results have yet to be fully established. The open source R library, quicR, was developed to fill this lack of standardization. This library provides functions, which can be easily integrated into existing R workflows, for data curation, analysis, and vizualiztion.
keywords: quicR, R, library, RT-QuIC, prion, diagnostics, CJD, Parkinson's
format: 
  pdf:
    papersize: letter
    documentclass: article
    fontsize: 12pt
    geometry:
      - top=20mm
      - right=20mm
      - bottom=20mm
      - left=20mm
    fig-pos: "H"
    include-in-header: 
      - text: \usepackage{tabularray}
      - text: \usepackage{float}
      - text: \floatplacement{table}{H}
mainfont: Calibri
bibliography: references.bib
bibliographystyle: apa
---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(quicR)
library(dplyr)
library(tidyr)
library(kableExtra)
library(readxl)
library(readr)
library(ggplot2)
library(reshape2)

file <- "../../inst/extdata/input_files/test2.xlsx"
file_384 <- "../../inst/extdata/input_files/test384.xlsx"
sample_file <- "../../inst/extdata/BMG_formatting/plate_layout.csv"
path <- "../../inst/extdata/BMG_formatting"
```


# Introduction
Real-time quaking induced conversion (RT-QuIC) is a cutting-edge diagnostic assay that has garnered significant attention for its ability to detect misfolded protein aggregates [@Atarashi2011]. The assay works by converting a recombinant protein substrate into an amyloid aggregate in the presence of a misfolded seed [@Wilham2010; @Orru2012; @Bongianni2019; @Orru2017; @Orru2015; @Dassanayake2016; @Hwang2018; @Groveman2018; @Metrick2020]. The assay's sensitivity and specificity make RT-QuIC a promising tool for diagnosing diseases such as prion disorders and other protein misfolding pathologies [@Fiorini2020; @Franceschini2017; @Picasso-Risso2022, @Holz2021]. However, the relatively recent development and novelty of the assay have left a gap in widely accepted academic and industry standards for data analysis and interpretation [@Rowden2023].

To address this gap, we introduce quicR, an open-source library, developed in R [@R2024], dedicated to the cleaning, analysis, and visualization of RT-QuIC data. By consolidating key metrics and providing robust analytical tools, quicR aims to standardize the analysis pipeline and foster reproducibility within the field of seeded amplification assays. quicR is designed with both researchers and diagnosticians in mind, providing a user-friendly interface that integrates seamlessly with existing R workflows.

While universal diagnostic criteria for RT-QuIC have yet to be established, certain analytical metrics have emerged as valuable tools for interpreting assay results and kinetics. These include:

-   Time-to-threshold (TtT): The time required for the fluorescence signal to exceed a predefined threshold [@Orru2015].
-   Rate of amyloid formation (RAF): A measure of the kinetics of aggregate growth, which provides insight into the relative quantity of misfolded seed [@Gallups2022].
-   Maxpoint ratio (MPR): A ratio-based metric measuring peak normalized fluorescence intensities [@Rowden2023].
-   Maximum slope (MS): The steepest rate of fluorescence increase, reflecting the most rapid phase of aggregation [@Henderson2015].

Together, these metrics enable researchers to characterize the kinetics of RT-QuIC reactions comprehensively, enhancing the rigor and reliability of diagnostic decisions.

In addition to analytical tools, quicR provides flexible and customizable visualization capabilities. Leveraging the powerful ggplot2 library [@ggplot2016], quicR enables users to generate high-quality, publication-ready figures. These visualizations can be further customized using the intuitive '+' syntax of ggplot2, allowing for tailored presentations of RT-QuIC data.

By combining standardized metrics, advanced visualization tools, and a commitment to open source science, quicR serves as a foundational resource for the growing RT-QuIC community. Its goal is to empower researchers to analyze and present their data with clarity, consistency, and cohesion.

# Methods

## Dependencies
This package requires the following dependencies: dplyr, ggplot2, stringr, tidyr, janitor, openxlsx, readxl, reshape2, and slider. Because the MARS software (BMG Labtech, Ortenberg, Germany) exports data as an Excel workbook, the packages, openxlsx [@openxlsx2024] and readxl [@readxl2023], were fundamental to performing downstream handling. The tidyverse packages (dplyr, ggplot2, stringr, and tidyr), were vital for writing easy-to-read code and for data visualization [@tidyverse2019]. The janitor package [@janitor2024] has data cleaning functions which were useful when importing data from Excel. The slider package provides tools which apply some function to a moving window which was crucial for determining the approximate derivative of raw data [@slider2024].

\newpage

## Input Formatting
MARS exports data into Excel workbooks. For many of the quicR functions to work together, the workbooks must be formatted correctly. In MARS, select "Excel Report", and a pop-up window will appear (see [@fig-mars]). The following options must be checked for proper output: "Microplate View", "Table view", "Transpose table", and "Add test run information". After clicking "Export report to Excel", an Excel workbook is created such as in [@fig-marsoutput].

![Excel export settings in MARS](images/MARS_settings.png){#fig-mars height=50%}

:::{#fig-marsoutput layout-nrow=2}
![Microplate view Excel spreadsheet. This is the first spreadsheet in the workbook and contains the plate layouts of any data the user exported from MARS. Typically, there is a header containing metadata followed by a number of named matrices.](images/sheet1.png){#fig-sheet1 height=50%}

![Table view Excel spreadsheet. This is the second spreadsheet in the workboook and contains columns of each well and time points. Each cell is a raw fluorescent value. This spreadsheet will also contain a header, but was excluded from this figure.](images/sheet2.png){#fig-sheet2 height=50%}

Excel spreadsheet exported from MARS.  
:::

## Key Metrics and Calculations
quicR has functions for calculating TtT, MPR, and MS. A graphical representation of these can be found in [@fig-metrics]. There is no dedicated function for RAF since it can simply be expressed as the inverse of TtT, and can therefore be calculated separately as in [@tbl-metrics].

TtT is calculated by iterating through each sample until a value is greater than the supplied threshold. It then determines the intersection between the previous and current read times and the threshold. If no value was found larger than the threshold, the total reaction run-time is returned.

MPR is defined as the maximum fluorescence divided by the background fluorescence. Thus, in order to calculate, the raw data must first be normalized against the background. This is done by choosing a cycle for background determination, and then dividing each read by that value. Once this is done, the MPR is taken as the max value.

Finally, MS is determined by approximating the maximum of the derivative of the raw data and is typically reported in units of $\Delta$RFU/h (i.e. the change in relative fluorescent units per hour). Originally, this was accomplished by applying a linear regression to a sliding window; however, this proved to be very computationally expensive. Now, the slopes are calculated using differences between two data points within the range of the sliding window. While this slightly reduces the accuracy of the approximation, it was decided that the improvement in computation time was worth the loss in accuracy.

![Example graphs highlighting the different calculated metrics. The red curve represents a raw data curve that has been normalized against background. MPR is calculated as the maximum fluorescent value achieved in the normalized raw data. TtT is determined as the time required to cross a given threshold (in this example, the threshold is 0.2). The blue curve represents the derivative of the raw data, and max slope is determined as the maximum of the derivative.](images/metric_example.png){#fig-metrics}

# Development
![Workflow hierarchy of the quicR package. Blue nodes indicate steps where BMG software is needed. Purple nodes indicate functions dedicated to handling metadata. Red nodes are functions that acquire and manipulate raw data. Orange nodes are functions which calculate some metric. Finally, yellow nodes represent endpoints in the data analysis.](images/workflow2.png){#fig-workflow}

quicR was developed to address the need for efficient data conversion and analysis of RT-QuIC data. The functions were designed with usability and reproducibility in mind, ensuring compatibility between multiple labs. Currently, the package accepts data exported from the proprietary MARS software (BMG Labtech, Ortenberg, Germany) as an Excel workbook.

This project was devloped using Git version control. GitHub workflows were utilized to ensure that the package is able to be installed on current operating systems. Additionally, tests were performed on each function using the testthat package [@testthat2011].

The functionality in this package revolves around data curation, metric calculations, and visualization. 

# Implementation
The implementation of the quicR package encompasses several streamlined processes designed to facilitate data input, cleaning, transformation, and analysis of real-time fluorescence data. This section provides a comprehensive guide to utilizing the package's key functionalities, detailing how to:

1.  Format and input sample data into Omega control software (BMG Labtech, Ortenberg, Germany).
2.  Extract, clean, and organize metadata and raw fluorescence data.
3.  Apply transformations and normalization to raw data for downstream analysis.
4.  Calculate critical analytical metrics, such as time-to-threshold (TtT), rate of amyloid formation (RAF), maxpoint ratio (MPR), and maximum slope (MS).

These steps are designed to enhance reproducibility, minimize manual data handling, and enable seamless integration with the MARS software workflow. Through practical examples, this section illustrates how each function operates, along with expected input and output formats, ensuring clarity and ease of use for researchers.

## Input of Sample IDs into Omega Control Software
The Omega control software allows input of a TXT file containing sample IDs, dilution factors, and their well locations. This file is uniquely formatted, and not easily reproduced manually. The function, "BMG_format", allows for input of a CSV file containing the plate layout (see @tbl-layout for proper formatting), and exports the formatted TXT file. The file can then be imported into the control software before running.

:::{#tbl-layout tbl-pos="ht"}
```{=latex}
\begin{tblr}{
  colspec = {|c|cccccccccccc|}, 
  row{1} = {font=\bfseries}, 
  column{1} = {font=\bfseries}, 
  rowhead = 1
}
  \hline
   & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\ 
  \hline
  A & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  B & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  C & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  D & P & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  E & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  F & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  G & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  H & N & S01 & S02 & S03 & S04 & S05 & S06 & S07 & S08 & S09 & S10 & S11 \\ 
  \hline
\end{tblr}
```
Example CSV file plate layout for input into the "BMG_format" function. The top left corner should be cell "A1" in the CSV file. The top numbered row and the left-most lettered column should never be altered.
:::

\newpage

### Formatted Plate Layout for MARS Input
The function, "BMG_format", includes the logical argument "write_file". If TRUE, it will create a TXT file. The path can be given to the "save_path" argument, and the file name can be supplied to the "save_name" argument. The text file will be formatted as follows, and can be imported into MARS.

```{r, eval=FALSE, message=FALSE}
BMG_format(sample_file, write_file = TRUE)
```
```
A1  P      P
B1  P      P
C1  P      P
D1  P      P
E1  N      N
F1  N      N
G1  N      N
H1  N      N
A2  X1     S01
B2  X1     S01
C2  X1     S01
D2  X1     S01
E2  X1     S01
F2  X1     S01
G2  X1     S01
H2  X1     S01
```

## Data Cleaning and Transformation
The MARS software (BMG Labtech, Ortenberg, Germany) exports real-time data as an Excel workbook. Typically, the first sheet in the workbook will include microplate views of both raw data and metadata; however, the metadata on this page is what will be most useful. Those tables are the "Sample IDs" and the "Dilutions" tables (if dilutions were included in the export). For much of the downstream analysis, it is crucial the the "Sample IDs" table was exported. If there is no table, the user can simply add it manually (see [@fig-sheet1] for proper formatting).

### Retrieving Metadata
The metadata is defined as either sample-dependent or -independent. Sample-dependent metadata includes information such as sample IDs and dilution factors, whereas sample-independent metadata includes the date, time, reaction ID, etc.

The dependent metadata can be retrieved using the "organize_tables" and "convert_tables" functions. The former returns a list of tables, and the latter converts each table into a column in a single data frame. See @tbl-tables and @tbl-converted for example outputs of these functions.
\newpage
```{r, echo=FALSE, message=FALSE}
tabs <- organize_tables(file)[-2]
```
```{r, eval=FALSE}
organize_tables(file)
```

```{r, echo=FALSE, as.is=TRUE}
#| label: tbl-tables
#| tbl-cap: Sample IDs and Dilution Factors. Note that these are the same tables shown in [@fig-sheet1].
rename <- function(x) {
  names(x) <- 1:12
  rownames(x) <- LETTERS[1:8]
  return(x)
}

tabs$`Sample IDs` %>%
  as.data.frame() %>%
  rename() %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    linesep = "",
    align = rep("c", 12)
  ) %>%
  column_spec(1, border_left = TRUE, border_right = TRUE, bold = TRUE) %>%
  column_spec(13, border_right = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  kable_styling(full_width = TRUE, font_size = 10)

tabs$Dilutions %>%
  as.data.frame() %>%
  rename() %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    linesep = "",
    align = rep("c", 12)
  ) %>%
  column_spec(1, border_left = TRUE, border_right = TRUE, bold = TRUE) %>%
  column_spec(13, border_right = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  kable_styling(full_width = TRUE, font_size = 10)
```

```{r, eval=FALSE}
organize_tables(file) %>% convert_tables()
```
```{r, echo=FALSE, message=FALSE}
#| label: tbl-converted
#| tbl-cap: The tables extracted from the "organize_tables" function are converted into columns in a dataframe. This format is much more compatible with downstream analysis.
tabs <- convert_tables(tabs)
tabs %>% head(12) %>% kable(row.names = FALSE, align = c("c", "c"))
```

\newpage

### Sample Locations
Samples locations can be extracted based on their well ID. The "get_sample_locations" function accepts additional arguments if dilution factors were exported from MARS. The "dilution_fun" argument will supply a function for transforming the dilution factors (e.g. if the user would want to perform a log transformation). This output of this function is critical as an argument in "plate_view" function which is further explained in the visualization section.

```{r, eval=FALSE}
get_sample_locations(
  file, 
  dilution_bool = TRUE, 
  dilution_fun = function(x) 
    -log10(x), 
  sep = " "
)
```
```{r, echo=FALSE}
get_sample_locations(
  file, dilution_bool = TRUE, dilution_fun = function(x) -log10(x), sep = " "
) %>%
  head(12) %>%
  kable(row.names = FALSE, align = c("c", "c"))
```

### Sample-Independent Metadata
The independent metadata can be retrieved using the "get_meta" function. This data is included in the header of the excel workbook.

```{r, eval=FALSE}
get_meta(file)
```
```{r, echo=FALSE, message=FALSE}
get_meta(file) %>%
  mutate_at("Meta_info", ~gsub("\\\\", "/", .)) %>%
  kable(row.names = FALSE)
```

## Retrieving and Manipulating Raw Data
The raw, real-time data is typically found on the second sheet of the Excel workbook. There are three functions dedicated to the retrieval and cleaning of raw data.
1.  get_real: Retrieves the raw data from the Excel file, and outputs it as a dataframe.
2.  transpose_real: Swaps the rows and columns which makes some downstream analyses easier.
3.  normalize_RFU: normalizes the raw data by dividing each read by background fluorescence at a given cycle.

### Retrieve Raw Data
Raw data can be retrieved using the "get_real" function. The logical argument, "ordered", indicates whether the user would prefer the columns to be ordered by well or by sample ID. By default, it is FALSE which will order the data by well. This should almost always be the case for easier integration with other downstream functions. Additionally, since there can be more than one instance of real-time data (depending on if the user added some calculations in MARS), "get_real" returns a list of dataframes. Therefore, the output should be indexed to access the data frame of interest.
```{r, eval=FALSE}
get_real(file)[[1]]
```
```{r, echo=FALSE}
columns <- 8
df_ <- get_real(file, ordered = FALSE)[[1]] %>% as.data.frame()
df_[1:11, 1:columns] %>% kable(row.names = FALSE, align = rep("c", columns))
```

\newpage

### Transpose Raw Data
This data is structured such that each sample is its own column (variable) and each row (observation) is a time point. While this format is technically correct, a transposed format is more ideal for some downstream manipulation. This operation is performed using the function, "transpose_real". After transposition, each time point is an individual column (variable), and each sample is an individual row (observation).

```{r, eval=FALSE}
get_real(file)[[1]] %>% transpose_real()
```
```{r, echo=FALSE}
columns <- 12
transpose_real(df_)[1:7, 1:columns] %>% 
  kable(row.names = FALSE, align = rep("c", columns))
```

### Normalize Raw Data
The function "normalize_RFU" will convert the raw data into a background normalized data set. The function includes two additional arguments, "bg_cycle" (the cycle which will be used as the background fluorescence value) and "transposed" (if FALSE, will make a call to the "transpose_real" function). Note that the fourth time point is all "1's" since this was designated the background cycle.

```{r, eval=FALSE}
get_real(file)[[1]] %>% normalize_RFU(transposed = FALSE)
```
```{r, echo=FALSE, message=FALSE}
df_norm <- normalize_RFU(df_, bg_cycle = 4, transposed = FALSE)

df_norm[1:7, 1:columns] %>%
  mutate_at(2:ncol(.), ~round(as.numeric(.), 2)) %>%
  kable(row.names = FALSE, align = rep("c", columns))

df_norm <- df_norm %>%
  mutate(`Sample IDs` = tabs$`Sample IDs`)
```

\newpage

## Calculations
There are three analytical metrics with dedicated functions: time-to-threshold (TtT), maxpoint ratio (MPR), and maximum slope (MS). The rate of amyloid formation does not have a designated function since it is simply the inverse of the time-to-threshold. Each function below accepts input from the "transpose_real" or the "normalize_RFU" functions. See @tbl-metrics for an example of the output of these functions.

### Thresholds
Many publications have different methods of determining thresholds. By and large, the most popular method is to take the average background fluorescence of the entire plate and add some multiple of standard deviations. The quicR package provides the "calculate_threshold" function for this purpose. A value can be provided as the optional argument, "multiplier", which will be applied to the standard deviation.
```{r, eval=FALSE}
get_real(file)[[1]] %>% 
  calculate_threshold(method = "stdev", multiplier = 10)
```
```{r, echo=FALSE}
calculate_threshold(df_, method = "stdev", multiplier = 10)
```

### Time-to-threshold & Rate of Amyloid Formation
TtT is calculated using the "calculate_TtT" function. The function must be supplied a threshold; default value is 2 (i.e. twice the background fluorescence if the data is normalized). A starting column should also be given as an integer; default value is 3. This is essentially asking how many columns of metadata are included before the fluorescence reads begin. 

TtT is calculated by iterating through each row and checking if a value is greater than the threshold. If the value is greater, the slope of the previous time-point to the current time-point is calculated, and the time intersection of the current read is returned.

### Maxpoint Ratio
MPR is calculated by the "calculate_MPR" function. Data must be normalized in order to derive this metric. In a normalized data set, the MPR is simply the maximum value achieved during the run. Raw data can be passed to this function, but the argument, "data_is_norm", must be set to TRUE. This will pass the raw data to "normalize_RFU" before calculating the MPR values.

### Maximum Slope
MS is calculated by the "calculate_MS" function. The function iterates through each row using a rolling window which can be adjusted (default value is 3). Given the window size, the slope is calculated based on change in fluorescence divided by the range of the window. The MS is simply the largest slope value recorded. The units are typically reported as $\Delta$RFU/h.

\newpage

```{r, eval=FALSE}
df_norm <- get_real(file) %>% normalize_RFU()

data.frame("Sample IDs" = tabs$`Sample IDs`) %>%
  mutate(
    Dilutions = -log10(tabs$dilutions),
    MPR = calculate_MPR(df_norm, start_col = 3, data_is_norm = TRUE),
    MS  = calculate_MS(df_norm, data_is_norm = TRUE),
    TtT = calculate_TtT(df_norm, threshold = 2, start_col = 3),
    RAF = 1 / TtT
  )
```

```{r, echo=FALSE}
#| label: tbl-metrics
#| tbl-cap: Calculated metrics
#| tbl-cap-location: top
df_analyzed <- data.frame("Sample IDs" = tabs$`Sample IDs`, check.names = FALSE) %>%
  mutate(
    Dilutions = -log10(tabs$Dilutions %>% na.omit() %>% as.numeric()),
    MPR = calculate_MPR(df_norm, start_col = 3, data_is_norm = TRUE) %>% round(2),
    MS = calculate_MS(df_norm, data_is_norm = TRUE) %>% round(3),
    TtT = calculate_TtT(df_norm, threshold = 2, start_col = 3) %>% round(2),
    RAF = (1 / TtT) %>% round(4)
  )
df_analyzed%>%
  head(12) %>%
  kable(row.names = FALSE, align = rep("c", ncol(df_analyzed)))
```

\newpage

## Visualization

### Plate View
The "plate_view" function requires *un-transposed* data and sample locations as arguments. It also has an argument for plate type which will either be 96 or 384.
```{r, warning=FALSE}
#| fig-width: 12
#| fig-height: 8
sample_locations <- get_sample_locations(
  file, 
  dilution_bool = TRUE,
  dilution_fun = function(x) -log10(x)
)

plate_view(df_, sample_locations)
```

\newpage

### Summary Plots
```{r, warning=FALSE, message=FALSE}
#| fig-width: 8
#| fig-height: 6.5
df_analyzed %>%
  melt(id.vars = c("Sample IDs", "Dilutions")) %>%
  mutate_at("Dilutions", as.factor) %>%
  
  ggplot(aes(`Sample IDs`, value, fill = Dilutions)) +
    geom_boxplot() +
    facet_wrap(~variable, scales = "free") +
    theme(
      legend.position = "bottom",
      strip.text = element_text(face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
      axis.title = element_blank()
    )
```



# Usage

## Installation
```{r, eval=FALSE}
# Latest CRAN release
install.packages("quicR")

# Development version
devtools::install_github("gage1145/quicR")
```

# Validation & Performance

# Discussion

# Conclusion
quicR provides improved and standardized methods for analyzing RT-QuIC data. 

# Acknowledgments

# References
::: {#refs}
:::

